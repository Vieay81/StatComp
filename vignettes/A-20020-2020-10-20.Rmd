---
title: "A-20020-2020-10-20"
output: html_document
---
## Question
Exercises 5.13, 5.15, 6.4 and 6.5 (page 151 and 180, Statistical Computating with R).  

## Answer
#### Exercise 5.13  
**Solution:** First, $g(x)=\frac{x^2}{\sqrt{2\pi}}e^{-x^2/2},x>1$, the two importance functions I founed are
\[
f_1(x)=\frac{2}{\sqrt{2\pi}}e^{-x^2/2},x>1,
\]
and 
\[
f_2(x)=\frac{1}{\sqrt{2\pi}}e^{-x/2},x>1.
\]
From the plot below, we can see that the two importance functions are actually "close" to $g(x)$.
```{r}
x <- seq(1,5,0.01)
g <- function(x) {x^2*exp(-x^2/2)/sqrt(2*pi)}
f1 <- function(x) {2*exp(-x^2/2)/sqrt(2*pi)}
f2 <- function(x) {exp(-x/2)/sqrt(2*pi)}
gs <- c(expression(g(x)==x^2*e^{(-x^2/2)}/sqrt(2*pi)),
        expression(f[1](x)==2*e^{(-x^2/2)}/sqrt(2*pi)),
        expression(f[2](x)==e^{(-x/2)}/sqrt(2*pi)))
plot(x, g(x), type = "l", ylab = "")
lines(x, f1(x), col = "red", lty = 2)
lines(x, f2(x), col = "blue", lty = 3)
legend("topright", legend = gs,
           lty = 1:3, inset = 0.02, col = 1:3)
```            

Let 
\[
\theta=\int_1^\infty g(x)\,dx=\frac{x^2}{\sqrt{2\pi}}e^{-x^2/2}\,dx,
\]
then the estimates by importance sampling with the two different importance functions are
\[
\hat\theta_1=\frac{1}{n}\sum\limits_{i=1}^n\frac{g(X_i)}{f_1(X_i)}=\frac{1}{n}\sum\limits_{i=1}^n\frac{X_i
^2}{2},
\]
where $X_i$ has pdf $f_1(x)=\frac{2}{\sqrt{2\pi}}e^{-x^2/2},x>1$,
\[
\hat\theta_2=\frac{1}{n}\sum\limits_{i=1}^n\frac{g(X_i)}{f_2(X_i)}=\frac{1}{n}\sum\limits_{i=1}^n(X_i^2e^{-(X_i^2-X_i)/2}),
\]
where $X_i$ has pdf $f_1(x)=\frac{1}{\sqrt{2\pi}}e^{-x/2},x>1$.  
However, the variance of these two estimates are
\[
Var(\hat\theta_1)=\frac{1}{n}Var\left(\frac{g(X)}{f_1(X)}\right)=\frac{1}{n}\left\{\int_1^\infty \left[\frac{g(x)}{f_1(x)}\right]^2f_1(x)\,dx-\theta^2\right\}=\frac{1}{n}\left\{\int_1^\infty \frac{x^4}{2\sqrt{2\pi}}e^{-x^2/2}\,dx-\theta^2\right\},
\]
and 
\[
Var(\hat\theta_2)=\frac{1}{n}Var\left(\frac{g(X)}{f_2(X)}\right)=\frac{1}{n}\left\{\int_1^\infty \left[\frac{g(x)}{f_2(x)}\right]^2f_2(x)\,dx-\theta^2\right\}=\frac{1}{n}\left\{\int_1^\infty \frac{x^4}{\sqrt{2\pi}}e^{-x^2+x/2}\,dx-\theta^2\right\}.
\]
Hence, $Var(\hat\theta_1)<Var(\hat\theta_2)$.

#### Exercise 5.13
**Solution:** Use the stratified importance sampling, and choose $f(x)=e^{-x}/(1-e^{-1}),0<x<1$ as the importance function. Divide the integral interval $(0,1)$ into five subintervals $I_j=\{x:a_{j-1}\leq x\leq a_j\}$ with endpoints $a_0=0,a_j=F^{-1}(j/5)=-\log[j(e^{-1}-1)/5+1],j=1,...,k-1$, and $a_k=1$.    
Then on the $j^{th}$ subinterval variablesare generated from the pdf
\[
f_j(x)=\frac{5e^{-x}}{1-e^{-1}},\,\frac{j-1}{5}<x<\frac{j}{5}.
\]
To sum up, the steps of the algorithm by for this method are    
**Step1:** Let $j:=1$;       
**Step2:** For the $j^{th}$ subinterval, generate random variates $U_i\sim U(0,1),i=1,...,n/k$;      
**Step3:** Let $X_i=-\log[(e^{-1}-1)(U_i+j-1)/5+1]$, and return $\hat\theta_k=\frac{k}{n}\sum\limits_{i=1}^{n/k}\frac{1-e^{-1}}{1+X_i^2}$;     
**Step4:** if $k<5$, then go back to **Step2**, and return $\hat\theta=\frac{1}{5}\sum\limits_{j=1}^{5}\hat\theta_j$ otherwise.     
```{r}
n <- 10000
k <- 5
N <- 100
set.seed(2)
theta_ <- numeric(k)
est <- numeric(N)
for (i in 1:N)
{
  for (j in 1:k)
 {
  u <- runif(n/k)
  x <- -log((exp(-1)-1)*(u+j-1)/5+1)
  theta_[j] <- mean((1-exp(-1))/(1+x^2))
 }
 est[i] = mean(theta_)
}
theta <- mean(est)
sd <- sd(est)
```
The estimate by this method is `r theta`, and the standard error is `r sd`, which is much less than that by simple importance sampling.   

#### Exercise 6.4
Assume that $X$ is a random variable following lognormal distribution, then $\log X\sim N(\mu,\sigma^2)$. We have 
\[
\frac{\frac{1}{n}\sum\limits_{i=1}^n\log X_i-\mu}{S/\sqrt n}\sim t(n-1).
\]
Then the confidence interval of $\mu$ is 
\[
\frac{1}{n}\sum\limits_{i=1}^n\log X_i\pm t_{\alpha/2}S/\sqrt n,
\]
where $t_{\alpha/2}$ is the $\alpha/2$ upper quantile of t-distribution, and $S$ is the sample variance of $\log X$. Then $\widehat{CLC}=\frac{1}{n}\sum\limits_{i=1}^n\log X_i- t_{\alpha/2}S/\sqrt n$ and  $\widehat{ULC}=\frac{1}{n}\sum\limits_{i=1}^n\log X_i+ t_{\alpha/2}S/\sqrt n$.     
The EPC is 
\[
\frac{1}{N}\sum\limits_{i=1}^NI\{\widehat{CLC}_i\leq\mu\leq\widehat{ULC}_i\}.
\]
Now generate lognormal distribution variates $X_1,..,X_n$ with $\mu=0,\sigma^2=1$.
```{r}
set.seed(3)
n <- 20
mu <- 0
sigma <- 1
N <- 1000
LCL <- UCL <- numeric(N)
for (i in 1:N)
{
  x <- rlnorm(n, 0, 1)
  LCL[i] <- mean(log(x)) - qt(0.975,n-1)*sd(log(x))/sqrt(n)
  UCL[i] <- mean(log(x)) + qt(0.975,n-1)*sd(log(x))/sqrt(n)
}
ECP <- mean(LCL <= mu & UCL >= mu)
```
The EPC is `r ECP`, which actually equals $1-\alpha=0.95$.

#### Exercise 6.5
The confidence interval of the mean of normal variable is 
\[
\bar X\pm t_{\alpha/2}S/\sqrt n,
\]
where $t_{\alpha/2}$ is the $\alpha/2$ upper quantile of t-distribution, and $S$ is the sample variance of $X$. Then $\widehat{CLC}=\bar X- t_{\alpha/2}S/\sqrt n$ and  $\widehat{ULC}=\bar X+t_{\alpha/2}S/\sqrt n$.     
The EPC is 
\[
\frac{1}{N}\sum\limits_{i=1}^NI\{\widehat{CLC}_i\leq\mu\leq\widehat{ULC}_i\}.
\]
```{r}
set.seed(4)
n <- 20
mu <- 2
N <- 1000
LCL <- UCL <- numeric(N)
for (i in 1:N)
{
  x <- rchisq(n, 2)
  LCL[i] <- mean(x) - qt(0.975,n-1)*sd(x)/sqrt(n)
  UCL[i] <- mean(x) + qt(0.975,n-1)*sd(x)/sqrt(n)
}
ECP <- mean(LCL <= mu & UCL >= mu)
```
The EPC is `r ECP` when $X\sim\chi^2(2)$, which is much lower than $1-\alpha=0.95$.