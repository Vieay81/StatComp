---
title: "Homeworks"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Homeworks}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---
Example #1 (Regression)
```{r}
ctl <- c(4.17,5.58,5.18,6.11,4.50,4.61,5.17,4.53,5.33,5.14)
trt <- c(4.81,4.17,4.41,3.59,5.87,3.83,6.03,4.89,4.32,4.69)
group <- gl(2, 10, 20, labels = c("Ctl","Trt"))
weight <- c(ctl, trt)
# estimated by OLS
lm.D9 <- lm(weight ~ group)
summary(lm.D9)
```
The $R^2$ is `r summary(lm.D9)$r.squared`
```{r}
# residual plots
plot(lm.D9)
```

Example #2  
Some information of dataset iris is shown below:
```{r iris}
xtable::xtable(head(iris))
```

Example #3  
Taylor expansion:  
$f(x)=f(x_0)+f'(x_0)(x-x_0)+\frac{f''(x_0)}{2!}(x-x_0)^2+\cdots+\frac{f^{(n)}(x_0)}{n!}(x-x_0)^n+R_n(x)$  
where $R_n(x)=\frac{f^{(n+1)}(\xi)}{(n+1)!}(x-x_0)^{n+1}$,$\xi\in(x_0,x)$

## Question
Exercises 3.3, 3.9, 3.10, and 3.13 (pages 94-95, Statistical Computating with R).  

## Answer
#### Exercise 3.3  
**Solution:** First, derive the inverse of cdf $F(x)$ of Pareto($a$,$b$). 
\begin{align*}
& y=F(x)=1-\left(\frac{b}{x}\right)^a,\quad x\ge b>0,a>0 \\
& \Rightarrow \left(\frac{b}{x}\right)^a=1-y \\
& \Rightarrow x=b(1-y)^{-1/a} \\
& \Rightarrow F^{-1}(y)=b(1-y)^{-1/a},\quad 0\leq y\leq 1
\end{align*}
The algorithm steps for inverse transform method are as follows: $(a=2,b=2)$  
**Step1:** Generate $u_i∼U[0,1],\,i=1,2,..,n$;  
**Step2:** Return $X_i:=F^{-1}(u_i)=,2(1-u_i)^{-1/2},\,i=1,2,..,n$.
```{r}
set.seed(1)
a <- 2
b <- 2
n <- 1000
u <- runif(n)
x <- 2*(1-u)^(-1/2)
hist(x, prob = TRUE, main = expression(f(x)==8*x^{-3}))
y <- seq(2, 100, .01)
lines(y, 8*y^(-3))
```
  
#### Exercise 3.9
**Solution:** The algorithm steps are as follows:  
**Step1:** At the $kth$ loop, generate i.i.d. $u_{k1},u_{k2},u_{k3}∼U(-1,1)$;  
**Step2:** If $|u_{k3}|\ge|u_{k2}|$ and $|u_{k3}|\ge|u_{k1}|$, return $X_k:=u_{k2}$; otherwise return $X_k:=u_{k3}$;  
**Step3:** Terminate the algorithm if $k=n$, or $k:=k+1$ and return **Step1**.
```{r}
set.seed(2)
n <- 10000
u <- matrix(runif(3*n,-1,1), nrow = n)
x <- numeric(n)
L <- apply(abs(u),1,which.max) == 3
coord <- 3-L
for (i in 1:n)
{
  x[i] <- u[i,coord[i]]
}
hist(x, prob = TRUE, main = expression(f[e](x)==frac(3,4)*(1-x^2)))
y <- seq(-1, 1, .01)
lines(y, (3/4)*(1-y^2))
```

#### Exercise 3.10
**Proof:** Let $X$ be the variate generated by the algorithm in Exercise 3.19, then derive the cdf of $X$, $F(x)$.  
\begin{align*}
F(x) & =P(X\leq x) \\
& =P(U_2\leq x,|U_3|\ge|U_2|,|U_3|\ge|U_1|)+P(U_3\leq x,otherwise) \\
& =P(U_2\leq x,|U_3|\ge|U_2|,|U_3|\ge|U_1|)+P(U_3\leq x,(|U_3|<|U_2|)\cup(|U_3|<|U_1|)) \\
& =P(U_2\leq x,|U_3|\ge|U_2|,|U_3|\ge|U_1|)+P(U_3\leq x,|U_3|<|U_2|) \\
& +P(U_3\leq x,|U_3|<|U_1|)-P(U_3\leq x,|U_3|<|U_1|,|U_3|<|U_2|) \\
& +\frac{1}{8}\left[\int_{-1}^x\left(\left(\int_{-1}^{-|u_2|}+\int_{|u_2|}^1\right)\left(\int_{-|u_3|}^{|u_3|}\,du_1\right)\,du_3\right)\,du_2+2\int_{-1}^x\left(\left(\int_{-1}^{-|u_3|}+\int_{|u_3|}^1\right)\,du_2\right)\,du_3
-\int_{-1}^x\left(\left(\int_{-1}^{-|u_3|}+\int_{|u_3|}^1\right)\,du_1\right)^2\,du_3\right] \\
& =-\frac{1}{4}x^3+\frac{3}{4}x+\frac{1}{2}
\end{align*}
Therefore, the pdf of $X$ is 
\[
f(x)=\partial F(x)/\partial x=-\frac{3}{4}x^2+\frac{3}{4}=f_e(x),
\]
that is, the algorithm given in Exercise 3.9 generates variates from the density $f_e$.  

#### Exercise 3.13
**Solution:** Use the inverse transform method. First, derive the inverse of cdf $F(x)$ of the mixture with $r$ and $\beta$. 
\begin{align*}
& x=F(y)=1-\left(\frac{\beta}{\beta+y}\right)^r,\quad y\ge 0 \\
& \Rightarrow \left(\frac{\beta}{\beta+y}\right)^r=1-x \\
& \Rightarrow \frac{\beta}{\beta+y}=(1-x)^{1/r} \\
& \Rightarrow y=\beta(1-x)^{-1/r}-\beta \\
& \Rightarrow F^{-1}(x)=\beta(1-x)^{-1/r}-\beta,\quad 0\leq x\leq 1
\end{align*}
The algorithm steps for inverse transform method are as follows: $(r=4,\beta=2,n=1000)$  
**Step1:** Generate $u_i∼U[0,1],\,i=1,2,..,n$;  
**Step2:** Return $X_i:=F^{-1}(u_i)=2(1-u_i)^{-1/4}-2,\,i=1,2,..,n$.
```{r}
set.seed(4)
r <- 4
beta <- 2
n <- 1000
u <- runif(n)
y <- beta*(1-u)^(-1/r) - beta
hist(y, prob = TRUE, main = expression(f(y)==frac(64,(2+y)^5)))
x <- seq(0, 10, .01)
lines(x, 64/((2+x)^5))
```

## Question
Exercises 5.1, 5.7 and 5.11 (pages 149-151, Statistical Computating with R).  

## Answer
#### Exercise 5.1  
**Solution:** First, we have
\[
\theta=\int_0^{\pi/3}\sin t\,dt=\int_0^{\pi/3}[(\pi/3)\sin t]\times (3/\pi)\,dt=E[(\pi/3)\sin X]=(\pi/3)E(\sin X),
\]
where $X\sim U[0,\pi/3]$.  
Therefore, an algorithm by Monte Carlo method is as follow:  
**Step1:** Generate $n$ random variates $U_i\sim U[0,1],i=1,2,...,n$;   
**Step2:** Let $X_i=(\pi/3)U_i,i=1,2,...,n$;    
**Step3:** Return $\hat\theta=\frac{\pi}{3n}\sum\limits_{i=1}^{n}\sin X_i$.
```{r}
set.seed(1)
n <- 10000
u <- runif(n)
x <- (pi/3)*u
theta_ <- (pi/3)*mean(sin(x))
theta <- 1 - cos(pi/3)
```
However, the exact value of the intergral is 
\[
\theta=\int_0^{\pi/3}\sin t\,dt=-\cos t\bigg|_0^{\pi/3}=\cos(0)-\cos(\pi/3)=1-0.5=0.5,
\]
and the estimate is $\hat\theta=$ `r theta_`.

### Exercise 5.7
**solution:** First, we have
\[
\theta=\int_0^1e^x\,dx=\int_0^1e^x\times 1\,dx=E(e^X),
\]
where $X\sim U[0,1]$.
Therefore, an algorithm by simple Monte Carlo method is as follow:  
**Step1:** Generate $n$ random variates $X_i\sim U[0,1],i=1,2,...,n$;   
**Step2:** Return $\hat\theta_1=\frac{1}{n}\sum\limits_{i=1}^{n}e^{X_i}$.   
On the other hand, an algorithm by antithetic variate approach  is as follow: 
**Step1':** Generate $n$ random variates $X_i\sim U[0,1],i=1,2,...,n$;   
**Step2':** Return $\hat\theta_2=\frac{1}{n}\sum\limits_{i=1}^{n/2}(e^{X_i}+e^{1-X_i})$.
```{r}
set.seed(2)
n <- 10000
x1 <- runif(n)
theta_1 <- mean(exp(x1))
x2 <- runif(n/2)
theta_2 <- mean(exp(x2)+exp(1-x2))/2
m <- 1000
MC1 <- MC2 <- numeric(m)
for (i in 1:m)
{
  x1 <- runif(n)
  x2 <- runif(n/2)
  MC1[i] <- mean(exp(x1))
  MC2[i] <- mean(exp(x2)+exp(1-x2))/2
}
var1 <- var(MC1)
var2 <- var(MC2)
theta <- exp(1)-1
```
However, the exact value of the intergral is 
\[
\theta=\int_0^1e^x\,dx=e^x\bigg|_0^1=e-1=1.718282,
\]
and the estimates by simple Monte Carlo method and by antithetic variate approach are $\hat\theta_1=$ `r theta_1` and $\hat\theta_2=$ `r theta_2`, respectively.   
Futhermore, the empirical variance of the estimates by simple Monte Carlo method and by antithetic variate approach are $var(\hat\theta_1)=$ `r var1` and $var(\hat\theta_2)=$ `r var2`, respectively. Then the antithetic variate approach achieved approximately `r (var1-var2)/var1` reduction in variance.    

## Exercise 5.11
**solution:** Obviously, the variance of $\hat\theta_c=c\hat\theta_1+(1-c)\hat\theta_2$ is 
\begin{align*}
var(\hat\theta_c) & =var(c\hat\theta_1+(1-c)\hat\theta_2) \\
& =c^2var(\hat\theta_1)+(1-c)^2var(\hat\theta_2)+2c(1-c)cov(\hat\theta_1,\hat\theta_2) \\
& =[var(\hat\theta_1)+var(\hat\theta_2)-2cov(\hat\theta_1,\hat\theta_2)]c^2+2[cov(\hat\theta_1,\hat\theta_2)-var(\hat\theta_2)]c+var(\hat\theta_2).
\end{align*}
Derivate $var(\hat\theta_c)$ respect to $c$ and make it equal to 0, we have 
\begin{align*}
& \frac{\partial var(\hat\theta_c)}{\partial c}=2[var(\hat\theta_1)+var(\hat\theta_2)-2cov(\hat\theta_1,\hat\theta_2)]c+2[cov(\hat\theta_1,\hat\theta_2)-var(\hat\theta_2)] \\
& \Rightarrow c* = \frac{var(\hat\theta_2)-cov(\hat\theta_1,\hat\theta_2)}{var(\hat\theta_1)+var(\hat\theta_2)-2cov(\hat\theta_1,\hat\theta_2)}=\frac{var(\hat\theta_2)-cov(\hat\theta_1,\hat\theta_2)}{var(\hat\theta_1-\hat\theta_2)}.
\end{align*}

## Question
Exercises 5.13, 5.15, 6.4 and 6.5 (page 151 and 180, Statistical Computating with R).  

## Answer
#### Exercise 5.13  
**Solution:** First, $g(x)=\frac{x^2}{\sqrt{2\pi}}e^{-x^2/2},x>1$, the two importance functions I founed are
\[
f_1(x)=\frac{2}{\sqrt{2\pi}}e^{-x^2/2},x>1,
\]
and 
\[
f_2(x)=\frac{1}{\sqrt{2\pi}}e^{-x/2},x>1.
\]
From the plot below, we can see that the two importance functions are actually "close" to $g(x)$.
```{r}
x <- seq(1,5,0.01)
g <- function(x) {x^2*exp(-x^2/2)/sqrt(2*pi)}
f1 <- function(x) {2*exp(-x^2/2)/sqrt(2*pi)}
f2 <- function(x) {exp(-x/2)/sqrt(2*pi)}
gs <- c(expression(g(x)==x^2*e^{(-x^2/2)}/sqrt(2*pi)),
        expression(f[1](x)==2*e^{(-x^2/2)}/sqrt(2*pi)),
        expression(f[2](x)==e^{(-x/2)}/sqrt(2*pi)))
plot(x, g(x), type = "l", ylab = "")
lines(x, f1(x), col = "red", lty = 2)
lines(x, f2(x), col = "blue", lty = 3)
legend("topright", legend = gs,
           lty = 1:3, inset = 0.02, col = 1:3)
```            

Let 
\[
\theta=\int_1^\infty g(x)\,dx=\frac{x^2}{\sqrt{2\pi}}e^{-x^2/2}\,dx,
\]
then the estimates by importance sampling with the two different importance functions are
\[
\hat\theta_1=\frac{1}{n}\sum\limits_{i=1}^n\frac{g(X_i)}{f_1(X_i)}=\frac{1}{n}\sum\limits_{i=1}^n\frac{X_i
^2}{2},
\]
where $X_i$ has pdf $f_1(x)=\frac{2}{\sqrt{2\pi}}e^{-x^2/2},x>1$,
\[
\hat\theta_2=\frac{1}{n}\sum\limits_{i=1}^n\frac{g(X_i)}{f_2(X_i)}=\frac{1}{n}\sum\limits_{i=1}^n(X_i^2e^{-(X_i^2-X_i)/2}),
\]
where $X_i$ has pdf $f_1(x)=\frac{1}{\sqrt{2\pi}}e^{-x/2},x>1$.  
However, the variance of these two estimates are
\[
Var(\hat\theta_1)=\frac{1}{n}Var\left(\frac{g(X)}{f_1(X)}\right)=\frac{1}{n}\left\{\int_1^\infty \left[\frac{g(x)}{f_1(x)}\right]^2f_1(x)\,dx-\theta^2\right\}=\frac{1}{n}\left\{\int_1^\infty \frac{x^4}{2\sqrt{2\pi}}e^{-x^2/2}\,dx-\theta^2\right\},
\]
and 
\[
Var(\hat\theta_2)=\frac{1}{n}Var\left(\frac{g(X)}{f_2(X)}\right)=\frac{1}{n}\left\{\int_1^\infty \left[\frac{g(x)}{f_2(x)}\right]^2f_2(x)\,dx-\theta^2\right\}=\frac{1}{n}\left\{\int_1^\infty \frac{x^4}{\sqrt{2\pi}}e^{-x^2+x/2}\,dx-\theta^2\right\}.
\]
Hence, $Var(\hat\theta_1)<Var(\hat\theta_2)$.

#### Exercise 5.13
**Solution:** Use the stratified importance sampling, and choose $f(x)=e^{-x}/(1-e^{-1}),0<x<1$ as the importance function. Divide the integral interval $(0,1)$ into five subintervals $I_j=\{x:a_{j-1}\leq x\leq a_j\}$ with endpoints $a_0=0,a_j=F^{-1}(j/5)=-\log[j(e^{-1}-1)/5+1],j=1,...,k-1$, and $a_k=1$.    
Then on the $j^{th}$ subinterval variablesare generated from the pdf
\[
f_j(x)=\frac{5e^{-x}}{1-e^{-1}},\,\frac{j-1}{5}<x<\frac{j}{5}.
\]
To sum up, the steps of the algorithm by for this method are    
**Step1:** Let $j:=1$;       
**Step2:** For the $j^{th}$ subinterval, generate random variates $U_i\sim U(0,1),i=1,...,n/k$;      
**Step3:** Let $X_i=-\log[(e^{-1}-1)(U_i+j-1)/5+1]$, and return $\hat\theta_k=\frac{k}{n}\sum\limits_{i=1}^{n/k}\frac{1-e^{-1}}{1+X_i^2}$;     
**Step4:** if $k<5$, then go back to **Step2**, and return $\hat\theta=\frac{1}{5}\sum\limits_{j=1}^{5}\hat\theta_j$ otherwise.     
```{r}
n <- 10000
k <- 5
N <- 100
set.seed(2)
theta_ <- numeric(k)
est <- numeric(N)
for (i in 1:N)
{
  for (j in 1:k)
 {
  u <- runif(n/k)
  x <- -log((exp(-1)-1)*(u+j-1)/5+1)
  theta_[j] <- mean((1-exp(-1))/(1+x^2))
 }
 est[i] = mean(theta_)
}
theta <- mean(est)
sd <- sd(est)
```
The estimate by this method is `r theta`, and the standard error is `r sd`, which is much less than that by simple importance sampling.   

#### Exercise 6.4
Assume that $X$ is a random variable following lognormal distribution, then $\log X\sim N(\mu,\sigma^2)$. We have 
\[
\frac{\frac{1}{n}\sum\limits_{i=1}^n\log X_i-\mu}{S/\sqrt n}\sim t(n-1).
\]
Then the confidence interval of $\mu$ is 
\[
\frac{1}{n}\sum\limits_{i=1}^n\log X_i\pm t_{\alpha/2}S/\sqrt n,
\]
where $t_{\alpha/2}$ is the $\alpha/2$ upper quantile of t-distribution, and $S$ is the sample variance of $\log X$. Then $\widehat{CLC}=\frac{1}{n}\sum\limits_{i=1}^n\log X_i- t_{\alpha/2}S/\sqrt n$ and  $\widehat{ULC}=\frac{1}{n}\sum\limits_{i=1}^n\log X_i+ t_{\alpha/2}S/\sqrt n$.     
The EPC is 
\[
\frac{1}{N}\sum\limits_{i=1}^NI\{\widehat{CLC}_i\leq\mu\leq\widehat{ULC}_i\}.
\]
Now generate lognormal distribution variates $X_1,..,X_n$ with $\mu=0,\sigma^2=1$.
```{r}
set.seed(3)
n <- 20
mu <- 0
sigma <- 1
N <- 1000
LCL <- UCL <- numeric(N)
for (i in 1:N)
{
  x <- rlnorm(n, 0, 1)
  LCL[i] <- mean(log(x)) - qt(0.975,n-1)*sd(log(x))/sqrt(n)
  UCL[i] <- mean(log(x)) + qt(0.975,n-1)*sd(log(x))/sqrt(n)
}
ECP <- mean(LCL <= mu & UCL >= mu)
```
The EPC is `r ECP`, which actually equals $1-\alpha=0.95$.

#### Exercise 6.5
The confidence interval of the mean of normal variable is 
\[
\bar X\pm t_{\alpha/2}S/\sqrt n,
\]
where $t_{\alpha/2}$ is the $\alpha/2$ upper quantile of t-distribution, and $S$ is the sample variance of $X$. Then $\widehat{CLC}=\bar X- t_{\alpha/2}S/\sqrt n$ and  $\widehat{ULC}=\bar X+t_{\alpha/2}S/\sqrt n$.     
The EPC is 
\[
\frac{1}{N}\sum\limits_{i=1}^NI\{\widehat{CLC}_i\leq\mu\leq\widehat{ULC}_i\}.
\]
```{r}
set.seed(4)
n <- 20
mu <- 2
N <- 1000
LCL <- UCL <- numeric(N)
for (i in 1:N)
{
  x <- rchisq(n, 2)
  LCL[i] <- mean(x) - qt(0.975,n-1)*sd(x)/sqrt(n)
  UCL[i] <- mean(x) + qt(0.975,n-1)*sd(x)/sqrt(n)
}
ECP <- mean(LCL <= mu & UCL >= mu)
```
The EPC is `r ECP` when $X\sim\chi^2(2)$, which is much lower than $1-\alpha=0.95$.

## Question
Exercises 6.7, 6.8, 6.C and a discussion (page 180-182, Statistical Computating with R).  

## Answer
#### Exercise 6.7  
**Solution:** The hypotheses of Skewness test of normality are
\[
H_0:\sqrt{\beta_1}=0;\quad H_1:\sqrt{\beta_1}\ne0.
\]
And the test statistic is
\begin{equation}
\sqrt{b_1}=\frac{\frac{1}{n}\sum_{i=1}^n(X_i-\bar X)^3}{(\frac{1}{n}\sum_{i=1}^n(X_i-\bar X)^2)^{3/2}},
\end{equation}
which is asymptotically normal with mean 0 and variance $\frac{6(n-2)}{(n+1)(n+3)}$, and large if the distribution of $X$ is unnormal.

Then the **Monte Carlo experiment to estimate power of a test against symmetric Beta(a,a) distributions (t(v) distributions)** is

1.Select the particular values of the parameter a of $Beta(a,a)$ ($v$ of $t(v)$).

2.For each replicate, indexed by $j=1,...,m$:

(a)Generate the $j^{th}$ random sample $x_1^{(j)},...,x_n^{(j)}$ from $Beta(a,a)$ (from $t(v)$).

(b)Compute the test statistic $\sqrt{b_1}$ from the $j^{th}$ sample.

(c)Record the test decision: set $I_j=1$ if $H_0$ is rejected at significance level $\alpha$, and otherwise set $I_j=0$.

3.Compute the proportion of significant test $\hat\pi(\theta_1)=\frac{1}{m}\sum_{j=1}^mI_j$.

```{r}
set.seed(1)
n <- 100
m <- 10000
a <- c(1, 5, 20) # alternatives
v <- c(1, 10, 100) # alternatives
alpha <- 0.05
cv <- qnorm(1-alpha/2, 0, sqrt(6*(n-2)/((n+1)*(n+3))))
sk <- function(x)
{
  x_ <- mean(x)
  m3 <- mean((x-x_)^3)
  m2 <- mean((x-x_)^2)
  m3/m2^1.5
}
power_beta <- power_t <- numeric(length(a))
for (i in 1:length(a))
{
  # Beta(a,a)
  sktests_beta <- replicate(m, expr = {
    x <- rbeta(n, a[i], a[i])
    as.integer(abs(sk(x)) >= cv)
  })
  power_beta[i] <- mean(sktests_beta)
  # t(v)
  sktests_t <- replicate(m, expr = {
    x <- rt(n, v[i])
    as.integer(abs(sk(x)) >= cv)
  })
  power_t[i] <- mean(sktests_t)
}
```
Choose the sets of $a=\{1,5,20\}$ and $v=\{1,10,100\}$, and the results are as follows:
|$Beta(a,a)$|Power|$t(v)$|Power|
|-|-|-|-|
|$a=1$|`r power_beta[1]`|$v=1$|`r power_t[1]`|
|$a=5$|`r power_beta[2]`|$v=10$|`r power_t[2]`|
|$a=20$|`r power_beta[3]`|$v=100$|`r power_t[3]`|

we can see that the powers of the tests with alternative $Beta(a,a)$ distributions are really small, that means the shape of $Beta(a,a)$ distribution is similar with that of normal distribution. But the powers of the tests with alternative $t(v)$ distributions are higher than those with alternative $Beta(a,a)$ distributions, which indicates that the shape of the distributions having heavy-tail are quit different with the shape of normal distribution. In addition, the power of the tests with alternative $t(v)$ distributions grows with the increase of $v$, since the asymptotical distribution of $t$-distribution is normal.

#### Exercise 6.8
**Solution:** The **Monte Carlo experiment to estimate power of Count Five tset (and $F$-tset) against a fixed alternative** is

1.Select the sample sizes represents small, medium and large.

2.For each replicate, indexed by $j=1,...,m$:

(a)Generate the $j^{th}$ random sample $x_1^{(j)},...,x_n^{(j)},y_1^{(j)},...,y_n^{(j)}$ from $N(0,1)$ and $N(0,1.5^2)$.

(b)Compute the test statistic $\max\{\sum_{i=1}^n(x_i^{(j)}>\max(y^{(j)}))+\sum_{i=1}^n(x_i^{(j)}<\min(y^{(j)})),\sum_{i=1}^n(y_i^{(j)}>\max(x^{(j)}))+\sum_{i=1}^n(y_i^{(j)}<\min(x^{(j)}))\}$ (and $S_x^2/S_y^2$) from the $j^{th}$ sample.

(c)Record the test decision: set $I_j=1$ if $H_0$ is rejected at significance level $\alpha$, and otherwise set $I_j=0$.

3.Compute the proportion of significant test $\hat\pi(\theta_1)=\frac{1}{m}\sum_{j=1}^mI_j$.
```{r}
set.seed(2)
sigma_1 <- 1
sigma_2 <- 1.5
n <- c(10, 50, 100)
alpha <- 0.055
m <- 10000
count5test <- function(x, y)
{
  X <- x - mean(x)
  Y <- y - mean(y)
  outx <- sum(X > max(Y)) + sum(X < min(Y))
  outy <- sum(Y > max(X)) + sum(Y < min(X))
  as.integer(max(c(outx, outy)) > 5)
}
power_5 <- power_F <- numeric(length(n))
for (i in 1:length(n))
{
  power_5[i] <- mean(replicate(m, expr = {
  x <- rnorm(n[i], 0, sigma_1)
  y <- rnorm(n[i], 0, sigma_2)
  count5test(x,y)
}))
  pvalues <- replicate(m, expr = {
  x <- rnorm(n[i], 0, sigma_1)
  y <- rnorm(n[i], 0, sigma_2)
  Ftest <- var.test(x, y)
  Ftest$p.value
})
  power_F[i] <- mean(pvalues <= alpha)
}
```
Choose the set of $n=\{10,50,100\}$, and the results are as follows:
|Sample size|Count Five test|$F$-test|
|-|-|-|
|$n=10$|`r power_5[1]`|`r power_F[1]`|
|$n=50$|`r power_5[2]`|`r power_F[2]`|
|$n=100$|`r power_5[3]`|`r power_F[3]`|
We can see that the powers of $F$-test are higher than those of Count Five test, which indicates that the $F$-test is not applicable for non-normal distribution.

#### Exercise 6.C
The hypotheses of Mardia's multivariate Skewness test of normality are
\[
H_0:\sqrt{\beta_{1,d}}=0;\quad H_1:\sqrt{\beta_{1,d}}\ne0.
\]
And the test statistic is
\begin{equation}
\sqrt{b_{1,d}}=\frac{1}{n^2}\sum_{i,j=1}^n((X_i-\bar X)^T\hat\Sigma^{-1}(X_j-\bar X))^3
\end{equation}
which is asymptotically chisquared with $d(d+1)(d+2)/6$ degrees of freedom, and large if the distribution of $X$ is unnormal.

Choose the multivariate normal distribution is that with parameter $\mu=(0,0)^T$ and $\Sigma=\left(\begin{smallmatrix} 1 & 0.5 \\ 0.5 & 1 \end{smallmatrix}\right)$.
```{r}
library(MASS)
set.seed(3)
# Example 6.8
n <- c(10, 20, 30, 50, 100, 500)
mu0 <- rep(0,2)
Sigma <- matrix(c(1,.5,.5,1), nrow = 2)
d <- length(mu0)
cv <- qchisq(c(1-.975,.975), d*(d+1)*(d+2)/6)
multisk <- function(x)
{
  X <- as.matrix(x)
  n <- dim(X)[1]
  Sigma <- (n-1)*var(X)/n
  S_1 <- solve(Sigma)
  X_ <- as.matrix(apply(X, 2, mean))
  b <- 0
  for (i in 1:n)
  {
    for (j in 1:n)
    {
      b <- b + (t(X[i,]-X_)%*%S_1%*%(X[j,]-X_))^3
    }
  }
  b/n^2
}
p_reject <- numeric(length(n))
m <- 1000
for (i in 1:length(n))
{
  p_reject[i] <- mean(replicate(m, expr = {
    x <- mvrnorm(n = n[i], mu0, Sigma)
    sk <- multisk(x)
    as.integer(n[i]*sk/6 < cv[1] | n[i]*sk/6 > cv[2])
  }))
}
```
The estimated Type I error rate are as follows:

`r print(rbind(n, p_reject))`

wihch are close to the nominal level $\alpha=0.05$.

Then estimate the power of the Mardia's multivariate Skewness test of normality against a contaminated normal alternative:
\[
(1-\epsilon)N\left(\left(\begin{smallmatrix}0\\0\end{smallmatrix}\right),\left(\begin{smallmatrix}1 & 0.5\\0.5 & 1\end{smallmatrix}\right)\right)+\epsilon N\left(\left(\begin{smallmatrix}0\\0\end{smallmatrix}\right),\left(\begin{smallmatrix}100 & 0.5\\0.5 & 100\end{smallmatrix}\right)\right),\quad0\leq\epsilon\leq1.
\]
```{r}
# Example 6.10
alpha <- .1
n <- 30
m <- 2500
mu1 <- mu2 <- rep(0,2)
Sigma1 <- matrix(c(1,.5,.5,1), nrow = 2)
Sigma2 <- matrix(c(100,.5,.5,100), nrow = 2) 
d <- length(mu0)
epsilon <- c(seq(0, .15, .01), seq(.15, 1, .05))
N <- length(epsilon)
power_multi <- numeric(N)
cv <- qchisq(c(alpha/2,1-alpha/2), d*(d+1)*(d+2)/6)
for (i in 1:N)
{
  e <- epsilon[i]
  power_multi[i] <- mean(replicate(m, expr = {
    index <- sample(c(1,2), replace = T, size = n, prob = c(1-e,e))
    x <- matrix(nrow = n, ncol = d)
    for (j in 1:n)
    {
      if (index[j] == 1)
        x[j,] <- mvrnorm(1, mu1, Sigma1)
      else
        x[j,] <- mvrnorm(1, mu2, Sigma2)
    }
    sk <- multisk(x)
    as.integer(n*sk/6 < cv[1] | n*sk/6 > cv[2])
  }))
}
plot(epsilon, power_multi, type = "b",
     xlab = bquote(epsilon), ylim = c(0,1))
abline(h = alpha, lty = 3)
se <- sqrt(power_multi*(1-power_multi)/m)
lines(epsilon, power_multi+se, lty = 3)
lines(epsilon, power_multi-se, lty = 3)
```

The empirical power curve is shown above. Note that the power curve crosses the horizontal line corresponding to $\alpha=0.10$ at both endpoints, $\epsilon=0$ and $\epsilon =1$ where the alternative is normally distributed. For $0\leq\epsilon\leq1$ the empirical power of the test is greater than 0.10 and highest when $\epsilon$ is about 0.15.

#### Discussion
First we can not consider the two powers are different directly through inexact equality.

(a)The corresponding hypothesis test problem is whether the two powers obtained by two different methods are different, that is , the hypothesess are:
\[
H_0:pw_1=pw_2;\quad H_1:pw_1\ne pw_2.
\]

(b)We should use Z-test, since the empirical power is a proportion that reject null hypothesis.

(c)Since the test statistic is 
\[
Z=\frac{pw_1-pw_2}{\sqrt{\frac{pw_1(1-pw_1)}{n_1}+\frac{pw_2(1-pw_2)}{n_2}}},
\]
We need the sample sizes of two methods and the two estimated powers.

## Question
Exercises 7.1, 7.5, 7.8 and 7.11 (page 212-213, Statistical Computating with R).  

## Answer
#### Exercise 7.1  
**Solution:** The jackknife estimatie of the bias and the standard error are
\[
\widehat{bias}_J=(n-1)(\overline{\bar X^*}_{(\cdot)}-\bar X),
\]
and 
\[
\hat{se}_J=\sqrt{\frac{n-1}{n}\sum\limits_{i=1}^n(\bar X_{(i)}-\overline{\bar X^*}_{(\cdot)})^2}.
\]
```{r}
data(law, package = "bootstrap")
n <- nrow(law)
cor_ <- cor(law$LSAT, law$GPA)
cor_jack <- numeric(n)
for (i in 1:n) 
{
  cor_jack[i] <- cor(law$LSAT[-i], law$GPA[-i])
}
bias_jack <- (n-1) * (mean(cor_jack)-cor_) 
se_jack <- sd(cor_jack) / sqrt(n) * (n-1)
```
By calculation, the jackknife estimatie of the bias and the standard error are `r bias_jack` and `r se_jack` respectively.

#### Exercise 7.5
**Solution:** The standard normal bootstrap CI is 
\[
(\bar X-z_{\alpha/2}\hat{se}(\bar X),\bar X+z_{\alpha/2}\hat{se}(\bar X))
\],
the basic bootstrap CI is
\[
(2\bar X-\bar X^*_{1-\alpha/2},2\bar X-\bar X^*_{\alpha/2})
\],
the percentile bootstrap CI is
\[
(\bar X^*_{1-\alpha/2},\bar X^*_{\alpha/2})
\],
and the BCa is
\[
(\bar X^*_{\alpha1},\bar X^*_{\alpha2}),
\]
where 
\[
\alpha_1=\Phi\left(\hat z_0+\frac{\hat z_0+z_{\alpha/2}}{1-\hat a(\hat z_0+z_{\alpha/2})}\right),
\]
\[
\alpha_2=\Phi\left(\hat z_0+\frac{\hat z_0+z_{1-\alpha/2}}{1-\hat a(\hat z_0+z_{1-\alpha/2})}\right).
\]
\[
\hat z_0=\Phi^{-1}\left(\frac{1}{B}\sum\limits_{b=1}^B I(\bar X^{(b)}<\bar X)\right),
\]
\[
\hat a=\frac{\sum_{i=1}^n(\bar X_{(.)}-\bar X_{(i)})^3}{6\left[\sum\limits_{i=1}^n(\bar X_{(.)}-\bar X_{(i)})^2\right]^{3/2}}.
\]
```{r warning=FALSE}
library(boot)
data(aircondit, package = "boot")
set.seed(15798)
B <- 1000
boot_mean <- function(x, i) mean(x[i])
m <- boot(data = aircondit$hours, statistic = boot_mean, R = B)
CI <- boot.ci(m, type = c("norm", "basic", "perc", "bca"))
print(CI)
```
The intervals are all different, since they hey have different structures. The standard normal bootstrap CI is based on asymptotic normality, the basic bootstrap CI is based on the large sample property, the percentile bootstrap CI is by assuming $\bar X^*|X_1,...,X_n$ and $\bar X$ have approximately the same distribution, and the BCa is a bias-corrected and accelerated CI for percentile bootstrap CI.

#### Exercise 7.8
**Solution:** The jackknife estimatie of the bias and the standard error are similar with Exercise 7.1.
```{r warning=FALSE}
set.seed(54964)
data(scor, package = "bootstrap")
B <- 2000
n <- nrow(scor)
Sigma_ <- var(scor)
eigen_ <- eigen(Sigma_, symmetric = T)
theta_ <- eigen_$values[1] / sum(eigen_$values)
Jackknife <- function(data)
{
  theta_jack <- numeric(n)
  for (i in 1:n)
  {
    Sigma_jack <- var(scor[-i,])
    eigen_jack <- eigen(Sigma_jack, symmetric = T)
    theta_jack[i] <- eigen_jack$values[1] / sum(eigen_jack$values)
  }
  theta_jack
}
theta_jack <- Jackknife(data = scor)
bias_jack <- (n-1) * (mean(theta_jack)-theta_)
sd_jack <- sqrt((n-1) * mean((theta_jack-mean(theta_jack))^2))
```
By calculation, the jackknife estimatie of the bias and the standard error are `r bias_jack` and `r sd_jack` respectively.

#### Exercise 7.11
**Solution:** **Procedure to estimate prediction error by leave-two-out cross validation**

1.For $k=1,...,n-1,\,l=k+1,...,n$, let observation$(x_k,y_k),(x_l,y_l)$ be the test sample and use the remaining observations to fit the model.

(a)Fit the model(s) using only the $n-2$ observations in the training set, $(x_i,y_i),i\ne k,l$.

(b)Compute the predicted response $\hat y_j=\hat\beta_0+\hat\beta_1x_j,j=k,l$ for the test sample.

(c)Compute the squared prediction error $e_{k,l}^2=(y_k-\hat y_k)^2+(y_l-\hat y_l)^2$

2.Estimate the mean of the squared prediction errors $\hat\sigma_\epsilon^2=\frac{2}{n(n-1)}\sum_{k=1}^{n-1}\sum_{l=k+1}^ne_{k,l}^2$.
```{r message=FALSE, warning=FALSE}
library(lattice)
library(DAAG)
data(ironslag)
n <- length(ironslag$magnetic)
e1 <- e2 <- e3 <- e4 <- matrix(nrow = n*(n-1)/2, ncol = 2)
flag <- 1
for (i in 1:(n-1)) 
{
  for (j in (i+1):n)
  {
    y <- ironslag$magnetic[-c(i,j)]
    x <- ironslag$chemical[-c(i,j)]
    lm1 <- lm(y ~ x, data = ironslag)
    y_ <- predict(lm1, newdata = data.frame(x = ironslag$chemical[c(i,j)]))
    e1[flag,] <- ironslag$magnetic[c(i,j)] - y_

    lm2 <- lm(y ~ x + I(x^2), data = ironslag)
    y_ <- predict(lm2, newdata = data.frame(x = ironslag$chemical[c(i,j)]))
    e2[flag,] <- ironslag$magnetic[c(i,j)] - y_
    
    lm3 <- lm(log(y) ~ x, data = ironslag)
    y_ <- predict(lm3, newdata = data.frame(x = ironslag$chemical[c(i,j)]))
    e3[flag,] <- ironslag$magnetic[c(i,j)] - y_
    
    lm4 <- lm(log(y) ~ log(x), data = ironslag)
    y_ <- predict(lm4, newdata = data.frame(x = ironslag$chemical[c(i,j)]))
    e4[flag,] <- ironslag$magnetic[c(i,j)] - y_
    
    flag <- flag + 1
  }
}
cat("model1 =", mean(apply(e1^2, 1, sum)),
"model2 =", mean(apply(e2^2, 1, sum)),
"model3 =", mean(apply(e3^2, 1, sum)),
"model4 =", mean(apply(e4^2, 1, sum))
)
```
From the result, we can see the prediction error of model2 is the lowest, so model2 is the best.

## Question
Exercises 8.3 and experiments(page 243, Statistical Computating with R).  

## Answer
#### Exercise 8.3 
**Solution:**

**Approximate permutation test procedure**

1.Compute the observed test statistic $\hat\theta(X,Y)=\hat\theta(Z,\nu)=\max(c(outx,outy))$.

2.For each replicate, indexed $b=1,...,B$:

(a)Generate a random permutation $\pi_b=\pi(\nu)$.

(b)Compute the statistic $\hat\theta^{(b)}=\hat\theta^*(Z,\pi_b)$.

3.If large values of $\hat\theta$ support the alternative, compute the ASL (the empirical p-value) by
\[
\hat p=\frac{1+\sum_{b=1}^BI(\hat\theta^{(b)})\ge\hat\theta}{B+1}.
\]
For a lower-tail or two-tail test test test $\hat p$is computed in a similar way.

4.Reject $H_0$at significant level $\alpha$ if $\hat p\le\alpha$.

```{r}
counttest <- function(x, y)
{
  X <- x - mean(x)
  Y <- y - mean(y)
  outx <- sum(X > max(Y)) + sum(X < min(Y))
  outy <- sum(Y > max(X)) + sum(Y < min(X))
  max(c(outx, outy))
}

n1 <- 20
n2 <- 30
mu1 <- 1
mu2 <- 0
sigma1 <- sigma2 <- 1
B <- 1000
m <- 100
reject <- 0
for (i in 1:m)
{
  x1 <- rnorm(n1, mu1, sigma1)
  x2 <- rnorm(n2, mu2, sigma2)
  z <- c(x1, x2)
  t0 <- counttest(x1, x2)
  t <- replicate(B, expr = {
    k <- sample(1:length(z), size = length(x1), replace = F)
    x <- z[k]
    y <- z[-k]
    counttest(x, y)
  })
  p <- mean(c(t0, t) >= t0)
  reject <- reject + (p < .05)
}
reject / m
```
The output of reject proportion is close to $\alpha=0.05$, which indicates that the new method based on permutation test is applicable.

#### Experiment
```{r}
library(RANN)
library(boot)
library(energy)
library(Ball)
Tn <- function(z, ix, sizes,k) 
{
  n1 <- sizes[1]; n2 <- sizes[2]; n <- n1 + n2
  if(is.vector(z)) z <- data.frame(z,0);
  z <- z[ix, ];
  NN <- nn2(data = z, k = k+1)
  block1 <- NN$nn.idx[1:n1,-1]
  block2 <- NN$nn.idx[(n1+1):n,-1]
  i1 <- sum(block1 < n1 + .5); i2 <- sum(block2 > n1+.5)
  (i1 + i2) / (k * n)
}

# Unequal variances and equal expectations
n1 <- 50
n2 <- 50
mu1 <- 0
mu2 <- 0
sigma1 <- 1
sigma2 <- 4
m <- 1e3
k <- 3
R <- 999 
n <- n1 + n2
N = c(n1,n2)
eqdist.nn <- function(z,sizes,k)
{
  boot.obj <- boot(data = z, statistic = Tn, R = R, sim = "permutation", sizes = sizes, k = k)
  ts <- c(boot.obj$t0, boot.obj$t)
  p.value <- mean(ts >= ts[1])
  list(statistic = ts[1], p.value = p.value)
}
p.values <- matrix(NA, m ,3)
for(i in 1:m)
{
  x <- matrix(rnorm(n1, mu1, sigma1), ncol = 1)
  y <- matrix(rnorm(n2, mu2, sigma2), ncol = 1)
  z <- rbind(x,y)
  p.values[i,1] <- eqdist.nn(z, N, k)$p.value
  p.values[i,2] <- eqdist.etest(z, sizes = N, R = R)$p.value
  p.values[i,3] <- bd.test(x = x, y = y, R = 999, seed = i*1165)$p.va
}
alpha <- .05;
power1 <- colMeans(p.values < alpha)
```
```{r}
# Unequal variances and unequal expectations
n1 <- 50
n2 <- 50
mu1 <- 0
mu2 <- 5
sigma1 <- 1
sigma2 <- 4
m <- 1e3
k <- 3
R <- 999 
n <- n1 + n2
N = c(n1,n2)
eqdist.nn <- function(z,sizes,k)
{
  boot.obj <- boot(data = z, statistic = Tn, R = R, sim = "permutation", sizes = sizes, k = k)
  ts <- c(boot.obj$t0, boot.obj$t)
  p.value <- mean(ts >= ts[1])
  list(statistic = ts[1], p.value = p.value)
}
p.values <- matrix(NA, m ,3)
for(i in 1:m)
{
  x <- matrix(rnorm(n1, mu1, sigma1), ncol = 1)
  y <- matrix(rnorm(n2, mu2, sigma2), ncol = 1)
  z <- rbind(x,y)
  p.values[i,1] <- eqdist.nn(z, N, k)$p.value
  p.values[i,2] <- eqdist.etest(z, sizes = N, R = R)$p.value
  p.values[i,3] <- bd.test(x = x, y = y, R = 999, seed = i*7864)$p.va
}
alpha <- .05;
power2 <- colMeans(p.values < alpha)
```
```{r}
# Non-normal distributions
n1 <- 50
n2 <- 50
mu1 <- 0
mu2 <- 5
sigma1 <- 1
sigma2 <- 4
m <- 1e3
k <- 3
R <- 999 
n <- n1 + n2
N = c(n1,n2)
eqdist.nn <- function(z,sizes,k)
{
  boot.obj <- boot(data = z, statistic = Tn, R = R, sim = "permutation", sizes = sizes, k = k)
  ts <- c(boot.obj$t0, boot.obj$t)
  p.value <- mean(ts >= ts[1])
  list(statistic = ts[1], p.value = p.value)
}
p.values <- matrix(NA, m ,3)
for(i in 1:m)
{
  x <- matrix(rt(n1, df = 1), ncol = 1)
  y <- matrix(c(rnorm(n2/2, mu1, sigma1), rnorm(n2/2, mu2, sigma2)), ncol = 1)
  z <- rbind(x,y)
  p.values[i,1] <- eqdist.nn(z, N, k)$p.value
  p.values[i,2] <- eqdist.etest(z, sizes = N, R = R)$p.value
  p.values[i,3] <- bd.test(x = x, y = y, R = 999, seed = i*94654)$p.va
}
alpha <- .05;
power3 <- colMeans(p.values < alpha)
```
```{r}
# Unbalanced samples
n1 <- 50
n2 <- 5
mu1 <- 0
mu2 <- 5
sigma1 <- 1
sigma2 <- 4
m <- 1e3
k <- 3
R <- 999 
n <- n1 + n2
N = c(n1,n2)
eqdist.nn <- function(z,sizes,k)
{
  boot.obj <- boot(data = z, statistic = Tn, R = R, sim = "permutation", sizes = sizes, k = k)
  ts <- c(boot.obj$t0, boot.obj$t)
  p.value <- mean(ts >= ts[1])
  list(statistic = ts[1], p.value = p.value)
}
p.values <- matrix(NA, m ,3)
for(i in 1:m)
{
  x <- matrix(rnorm(n1, mu1, sigma1), ncol = 1)
  y <- matrix(rnorm(n2, mu2, sigma2), ncol = 1)
  z <- rbind(x,y)
  p.values[i,1] <- eqdist.nn(z, N, k)$p.value
  p.values[i,2] <- eqdist.etest(z, sizes = N, R = R)$p.value
  p.values[i,3] <- bd.test(x = x, y = y, R = 999, seed = i*78324)$p.va
}
alpha <- .05;
power4 <- colMeans(p.values < alpha)
```
The result is as follow:

|NN|Energy|Ball|
|---|---|---|
|`r power1[1]`|`r power1[2]`|`r power1[3]`|
|`r power2[1]`|`r power2[2]`|`r power2[3]`|
|`r power3[1]`|`r power3[2]`|`r power3[3]`|
|`r power4[1]`|`r power4[2]`|`r power4[3]`|

That indicates Energy test could deal with the Non-normal distributions problem and Unbalanced samples problem. In addition, the results of three methods are similar with the Unequal variances or expectations problem.

## Question
Exercises 9.4 and 11.4 (page 277 and 353, Statistical Computating with R).  

## Answer
#### Exercise 9.4 
```{r}
rw_Metropolis <- function(sigma, x0, N)
{
  x <- numeric(N)
  x[1] <- x0
  u <- runif(N)
  k <- 0
  for (i in 2:N)
  {
    y <- rnorm(1, x[i-1], sigma)
    if (u[i] <= exp(-abs(y))/exp(-abs(x[i-1])))
      x[i] <- y
    else
    {
      x[i] <- x[i-1]
      k <- k + 1
    }
  }
  list(x = x, k = k)
}

set.seed(324)
N <- 2000
sigma <- c(.05, .5, 2, 4)
x0 <- 0
rw1 <- rw_Metropolis(sigma[1], x0, N)
rw2 <- rw_Metropolis(sigma[2], x0, N)
rw3 <- rw_Metropolis(sigma[3], x0, N)
rw4 <- rw_Metropolis(sigma[4], x0, N)
plot(1:N, rw1$x, type = "l", xlab = expression(sigma==0.05))
plot(1:N, rw2$x, type = "l", xlab = expression(sigma==0.5))
plot(1:N, rw3$x, type = "l", xlab = expression(sigma==02))
plot(1:N, rw4$x, type = "l", xlab = expression(sigma==4))
accept <- 1 - c(rw1$k, rw2$k, rw3$k, rw4$k) / N
print(accept)
```
From the plots, we can see that the ratios $r(X_t,Y)$ with $\sigma=0.05$ tend to be large and almost every candidate point is accepted. The increments are small and the chain is almost like a true random walk. Chain 1 has not converged to the target in 2000 interations. The Chain 2 generated with $\sigma=0.5$ is converging slowly. The Chain 3 and Chain 4 are mixing well and converging to the targest distribution after a short burn-in period.

In addtion, the acceptance rates are `r accept` respectively, 

## Exercise 9.4(Cont.)
```{r}
GR <- function(psi)
{
  psi <- as.matrix(psi)
  n <- ncol(psi)
  k <- nrow(psi)
  m <- rowMeans(psi)
  B <- n * var(m)
  w <- apply(psi, 1, "var")
  W <- mean(w)
  v_ <- W*(n-1)/n + B/n
  r_ <- v_ / W
  r_
}

set.seed(324)
N <- 2000
burn <- 200
x0 <- c(-10, -5, 5, 10)
X <- matrix(nrow = 4, ncol = N)
for (i in 1:4)
{
  X[i,] <- rw_Metropolis(sigma[2], x0[i], N)$x
}
psi <- t(apply(X, 1, cumsum))
for (i in 1:nrow(psi))
  psi[i,] <- psi[i,] / (1:ncol(psi))
R_ <- GR(psi)
for (i in 1:4)
  plot(psi[i, (burn+1):N], type = "l", xlab = paste("x0 =", x0[i]), ylab = bquote(psi))
r_ <- numeric(N)
for (i in (burn+1):N)
  r_[i] <- GR(psi[,1:i])
plot(r_[(burn+1):N], type = "l", xlab = "", ylab = "R")
abline(h = 1.2, lty = 2)

N <- 5000
burn <- 500
x0 <- c(-10, -5, 5, 10)
X <- matrix(nrow = 4, ncol = N)
for (i in 1:4)
{
  X[i,] <- rw_Metropolis(sigma[2], x0[i], N)$x
}
psi <- t(apply(X, 1, cumsum))
for (i in 1:nrow(psi))
  psi[i,] <- psi[i,] / (1:ncol(psi))
for (i in 1:4)
  plot(psi[i, (burn+1):N], type = "l", xlab = paste("x0 =", x0[i]), ylab = bquote(psi))
r_ <- numeric(N)
for (i in (burn+1):N)
  r_[i] <- GR(psi[,1:i])
plot(r_[(burn+1):N], type = "l", xlab = "", ylab = "R")
abline(h = 1.2, lty = 2)
index <- min(which(r_[r_ != 0] < 1.2))
rw <- rw_Metropolis(sigma[2], x0[2], index+burn)
plot(1:(index+burn), rw$x, type = "l", xlab = "", main = "Runs until converges")
```

In this section we just show the work with only one of the Chains we got in Exercise 9.4, since the steps are the same for different $\sigma$. We choose Chain 3 with $\sigma=0.5$, and the second plot above indicates that the Chain did not converge within 2000 iterations ($\hat R>1.2$). The fourth plot shows that the Chain converges within 5000 iterations, then run the Chain for 5000 iterations.

## Exercise 11.4
```{r}
k <- c(4:25, 100, 500, 1000)
roots <- numeric(length(k))
for (i in 1:length(k))
{
  f <- function(a) pt(sqrt(a^2*(k[i]-1)/(k[i]-a^2)), df = k[i]-1) - pt(sqrt(a^2*k[i]/(k[i]+1-a^2)), df = k[i])
  roots[i] <- uniroot(f, c(1e-10,sqrt(k[i])-1e-10))$root
}
print(roots)
plot(1:length(k), type = "l", roots, xlab = "", ylab = expression(A(k)), main = "Intersection points")
```

## Question
A-B-O blood problem, Exercises 3, 3 and 6 (page 204, 213-214, Advanced R).  

## Answer
#### A-B-O blood problem 
The likelihood for full data is 
\[
L(p,q|n_{AA},n_{AO},n_{BB},n_{BO},n_{OO},n_{AB})=(p^2)^{n_{AA}}(2p(1-p-q))^{n_{AO}}(q^2)^{n_{BB}}(2q(1-p-q))^{n_{BO}}(1-p-q)^{2n_{OO}}(2pq)^{n_{AB}},
\]
and the corresponding log-likelihood is
\begin{align*}
& l(p,q|n_{AA},n_{AO},n_{BB},n_{BO},n_{OO},n_{AB}) \\
& =n_{AA}\log(p^2)+n_{AO}\log(2p(1-p-q))+n_{BB}\log(q^2)+n_{BO}\log(2q(1-p-q))+n_{OO}\log(1-p-q)^2+n_{AB}\log(2pq).
\end{align*}
**E-step:**
\begin{align*}
& E[l(p,q|n_{AA},n_{A.},n_{BB},n_{B.},n_{OO},n_{AB})|p^{(t)},q^{(t)}]\\
& =\frac{(p^{(t)})^2}{(p^{(t)})^2+2p^{(t)}(1-p^{(t)}-q^{(t)})}n_{A.}\log(\frac{p}{2(1-p-q)})+\frac{(q^{(t)})^2}{(q^{(t)})^2+2q^{(t)}(1-q^{(t)}-q^{(t)})}n_{B.}\log(\frac{q}{2(1-p-q)}) \\
& +2n_{OO}\log(1-p-q)+n_{A.}\log(2p(1-p-q))+n_{B.}\log(2q(1-p-q))+n_{AB}\log(2pq),
\end{align*}
since $n_{AA}|(p^{(t)},q^{(t)})\sim B(n_{A.},\frac{(p^{(t)})^2}{(p^{(t)})^2+2p^{(t)}(1-p^{(t)}-q^{(t)})}),n_{BB}|(p^{(t)},q^{(t)})\sim B(n_{B.},\frac{(q^{(t)})^2}{(p^{(t)})^2+2p^{(t)}(1-p^{(t)}-q^{(t)})}).$

**M-step:**
\[
(p^{(t+1)},q^{(t+1)})=\arg\max_{p,q}E[l(p,q|n_{AA},n_{A.},n_{BB},n_{B.},n_{OO},n_{AB})|p^{(t)},q^{(t)}].
\]
```{r warning=FALSE}
Eloglike <- function(p = p_t, q = q_t) 
    -(2*nOO*log(1-p-q) + nA.*log(p*(1-p-q)) + nB.*log(q*(1-p-q)) + nAB*log(p*q) + (p_t)/(p_t+2*r_t)*nA.*log(p/(1-p-q)) +   (q_t)/(q_t+2*r_t)*nB.*log(q/(1-p-q)))
likelihood <- function(p_t1, q_t1, r_t1, p_t, q_t, r_t)
  log(((p_t1/p_t)^2+2*(p_t1/p_t)*(r_t1/r_t))^nA. * ((q_t1/q_t)^2+2*(q_t1/q_t)*(r_t1/r_t))^nB. * ((r_t1/r_t)^2)^nOO * (2*(p_t1/p_t)*(q_t1/q_t))^nAB) 

library(stats4)
nA. <- 444
nB. <- 132
nOO <- 361
nAB <- 63
N <- 10000
p_t <- q_t <- r_t <- 1/3
eps <- .Machine$double.eps^.5
likelihood_o <- numeric(N)
for (i in 1:N)
{
  fit <- mle(Eloglike)
  p_t1 <- fit@coef[["p"]]
  q_t1 <- fit@coef[["q"]]
  r_t1 <- 1 - p_t1 - q_t1
  likelihood_o[i] <- likelihood(p_t1, q_t1, r_t1, p_t, q_t, r_t)
  res <- sum(abs(c(p_t1, q_t1, r_t1)-c(p_t, q_t, r_t)) / c(p_t, q_t, r_t))
  if (res < eps)
    break
  p_t <- p_t1
  q_t <- q_t1
  r_t <- r_t1
}
print(list("p" = p_t1, "q" = q_t1, "iterations" = i, "res" = res))
plot(2:i, likelihood_o[2:i], type = "l", xlab = "iteration", ylab = "log-likelihood difference", main = "The difference of log-likelihoods")
```

Since the log-likelihood is too small to output, we just plot the The differences of log-likelihoods, which indicates that all the differences are positive, meaning the log-likelihoods are increasing.

## Exercise 3
```{r}
data(mtcars)
formulas <- list(
mpg ~ disp,
mpg ~ I(1 / disp),
mpg ~ disp + wt,
mpg ~ I(1 / disp) + wt
)
# for loops
for (i in formulas)
{
  model <- lm(i, data = mtcars)
  print(summary(model))
}
# lapply
lm4 <- lapply(formulas, function(f) lm(f, data = mtcars))
print(lm4)
```

## Exercise 3
```{r}
set.seed(3)
random <- replicate(100, list(rpois(10, 10), rpois(7, 10)))
locs <- 1:100
p <- sapply(locs, function(i) t.test(random[[2*i-1]], random[[2*i]])$p.value)
print(p)
```

## Exercise 6
Here we give an example by the combination of Map() and vapply() to calculate the  standard-deviation of each variable. In this function, the kernel is **vapply(Map(std, x, m), unlist, numeric(1))**, in which all the arguments are needed. 
```{r}
data(mtcars)
std <- function(x, x_bar) sum((x-x_bar)^2) / (length(x)-1)
mapva <- function(x)
{
  m <- vapply(x, mean, numeric(1))
  vapply(Map(std, x, m), unlist, numeric(1))
}
mapva(mtcars)
```

## Question
Write an Rcpp function for Exercises 9.4 (page 277, Statistical Computating with R).  

## Answer
The C++ code is as below:
```
# include <Rcpp.h>
# include <stdlib.h>
# include <math.h> 
# define pi 3.1415926
using namespace Rcpp;

double randn()
{
  double u = ((double) rand()/(RAND_MAX));
  double v = ((double) rand()/(RAND_MAX));
  double z1 = sqrt((double)-2*log(u)) * cos((double)2*pi*v);
  if (isnan(z1))
    return (randn());
  else
    return (z1);
}

// [[Rcpp::export]]
NumericVector Metropolis(double x0, double sigma, int N) 
{
  NumericVector x(N);
  
  x[0] = x0;
  for (int i = 1; i < N; i++)
  {
    double y = (double)(sigma*randn() + x[i-1]);
    double u = ((double) rand()/(RAND_MAX));
    if (u <= (exp(-abs(y)) / exp(-abs(x[i-1]))))
      x[i] = y;
    else 
    x[i] = x[i-1];
  }
  return (x);
}
```

```{r}
library(Rcpp)
library(microbenchmark)
dir_cpp <- "../src/"
sourceCpp(paste0(dir_cpp, "Metropolis.cpp"))
rw_Metropolis <- function(x0, sigma, N)
{
  x <- numeric(N)
  x[1] <- x0
  u <- runif(N)
  for (i in 2:N)
  {
    y <- rnorm(1, x[i-1], sigma)
    if (u[i] <= exp(-abs(y))/exp(-abs(x[i-1])))
      x[i] <- y
    else
      x[i] <- x[i-1]
  }
  x
}

set.seed(123)
sigma <- 2
x0 <- 0
N <- 2000
xr <- rw_Metropolis(x0, sigma, N)
xcpp <- Metropolis(x0, sigma, N)
qqplot(xr[1001:2000], xcpp[1001:2000], main = "QQ plot")
```

The QQplot shows that the generated random numbers are in the same distribution, which means that the two algorithms could generate the same distributed random numbers. 
```{r}
ts <- microbenchmark(rw_Metropolis(x0, sigma, N), Metropolis(x0, sigma, N))
summary(ts)
```
The result indicates that the C++ function is much more effcient than the R function.

